<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Towards Video World Models</title>

    <!-- Open Graph / Facebook -->
    <meta property="og:type" content="article">
    <meta property="og:url" content="https://www.xunhuang.me/blogs/world_model.html">
    <meta property="og:title" content="Towards Video World Models">
    <meta property="og:description" content="Exploring the path from video generation to true video world models through causality, interactivity, persistence, real-time responsiveness, and physical accuracy.">
    <meta property="og:image" content="https://www.xunhuang.me/imgs/blog/pyramid.jpg">
    <meta property="og:image:width" content="1200">
    <meta property="og:image:height" content="630">
    <meta property="og:image:alt" content="Diagram showing internal world model in human brain vs external world model simulation">

    <!-- Twitter -->
    <meta property="twitter:card" content="summary_large_image">
    <meta property="twitter:url" content="https://www.xunhuang.me/blogs/world_model.html">
    <meta property="twitter:title" content="Towards Video World Models">
    <meta property="twitter:description" content="Exploring the path from video generation to true video world models through causality, interactivity, persistence, real-time responsiveness, and physical accuracy.">
    <meta property="twitter:image" content="https://www.xunhuang.me/imgs/blog/pyramid.jpg">
    <meta property="twitter:image:alt" content="Diagram showing internal world model in human brain vs external world model simulation">

    <!-- Additional meta tags -->
    <meta name="description" content="Exploring the path from video generation to true video world models through causality, interactivity, persistence, real-time responsiveness, and physical accuracy.">
    <meta name="author" content="Xun Huang">
    <meta name="keywords" content="world models, video generation, AI, machine learning, diffusion models, autoregressive models">
    <meta name="robots" content="index, follow">
    <meta name="language" content="English">
    <meta name="revisit-after" content="7 days">

    <!-- Canonical URL -->
    <link rel="canonical" href="https://www.xunhuang.me/blogs/world_model.html">
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            color: #333;
            background-color: #f8f9fa;
        }

        .container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 0 20px;
            display: flex;
            gap: 40px;
        }

        .main-content {
            flex: 1;
            max-width: 800px;
            background: white;
            padding: 40px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .toc-sidebar {
            width: 320px;
            position: sticky;
            top: 20px;
            height: fit-content;
            background: white;
            padding: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        .back-link {
            display: inline-block;
            margin-bottom: 20px;
            color: #007bff;
            text-decoration: none;
            font-weight: 500;
        }

        .back-link:hover {
            text-decoration: underline;
        }

        .back-link::before {
            content: "â† ";
        }

        h1 {
            font-size: 2.5em;
            margin-bottom: 20px;
            color: #2c3e50;
            border-bottom: 3px solid #00cc33;
            padding-bottom: 10px;
        }

        h2 {
            font-size: 1.8em;
            margin: 30px 0 15px 0;
            color: #34495e;
            border-left: 4px solid #00cc33;
            padding-left: 15px;
        }

        h3 {
            font-size: 1.4em;
            margin: 25px 0 10px 0;
            color: #2c3e50;
        }

        h4 {
            font-size: 1.2em;
            margin: 20px 0 10px 0;
            color: #34495e;
        }

        p {
            margin-bottom: 15px;
            text-align: justify;
        }

        ul, ol {
            margin: 15px 0;
            padding-left: 30px;
        }

        li {
            margin-bottom: 8px;
        }

        .toc-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 15px;
            color: #2c3e50;
            border-bottom: 2px solid #00cc33;
            padding-bottom: 5px;
        }

        .toc-list {
            list-style: none;
            padding: 0;
        }

        .toc-list li {
            margin-bottom: 8px;
        }

        .toc-list a {
            color: #555;
            text-decoration: none;
            font-size: 0.9em;
            line-height: 1.4;
            display: block;
            padding: 5px 0;
            border-radius: 4px;
            transition: all 0.2s ease;
        }

        .toc-list a:hover {
            background-color: #f8f9fa;
            color: #00cc33;
            padding-left: 8px;
        }

        .toc-list .toc-h2 {
            font-weight: 600;
            color: #2c3e50;
        }

        .toc-list .toc-h3 {
            margin-left: 15px;
            font-size: 0.85em;
        }

        .toc-list .toc-h4 {
            margin-left: 30px;
            font-size: 0.8em;
        }

        .highlight {
            background-color: #fff3cd;
            padding: 15px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
            border-radius: 4px;
        }

        .code-block {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 4px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
            overflow-x: auto;
        }

        blockquote {
            border-left: 4px solid #00cc33;
            padding-left: 20px;
            margin: 20px 0;
            font-style: italic;
            color: #555;
        }

        .matrix-quote {
            background-color: #0a0a0a;
            color: #00ff00;
            font-family: 'Courier New', monospace;
            padding: 25px;
            border-radius: 8px;
            margin: 30px 0;
            border: 2px solid #00ff00;
            font-style: normal;
            position: relative;
            box-shadow: 0 0 20px rgba(0, 255, 0, 0.3);
            animation: matrix-glow 3s ease-in-out infinite alternate;
        }

        .matrix-quote::before {
            content: '';
            position: absolute;
            top: -2px;
            left: -2px;
            right: -2px;
            bottom: -2px;
            background: linear-gradient(45deg, #00ff00, #008800, #00ff00);
            border-radius: 8px;
            z-index: -1;
            animation: matrix-border 2s linear infinite;
        }

        .matrix-quote .quote-text {
            font-size: 1.0em;
            line-height: 1.6;
            text-shadow: 0 0 10px rgba(0, 255, 0, 0.5);
            letter-spacing: 0.5px;
        }

        .matrix-quote .quote-author {
            text-align: right;
            margin-top: 15px;
            font-size: 0.9em;
            color: #00cc00;
            font-weight: bold;
        }

        @keyframes matrix-glow {
            0% {
                box-shadow: 0 0 20px rgba(0, 255, 0, 0.3);
            }
            100% {
                box-shadow: 0 0 30px rgba(0, 255, 0, 0.5);
            }
        }

        @keyframes matrix-border {
            0% {
                background: linear-gradient(45deg, #00ff00, #008800, #00ff00);
            }
            50% {
                background: linear-gradient(45deg, #008800, #00ff00, #008800);
            }
            100% {
                background: linear-gradient(45deg, #00ff00, #008800, #00ff00);
            }
        }

        img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            margin: 20px 0;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }

        figure {
            margin: 30px 0;
            text-align: center;
        }

        figure img {
            margin: 0;
        }

        figcaption {
            margin-top: 10px;
            font-size: 0.9em;
            color: #666;
            font-style: italic;
            line-height: 1.4;
        }

        .date {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 20px;
        }



        @media (max-width: 768px) {
            .container {
                flex-direction: column;
                gap: 20px;
            }

            .toc-sidebar {
                width: 100%;
                position: static;
            }

            .main-content {
                padding: 20px;
            }

            h1 {
                font-size: 2em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="main-content">
            <a href="../index.html" class="back-link">Back to Homepage</a>

            <div class="date">July 15, 2025</div>

            <h1>Towards Video World Models</h1>

            <p>
                The term "world model" has gained considerable popularity in 2025, yet it remains without a clear, universally agreed-upon definition. In this blog post, I will first disentangle the term by clarifying distinct paradigms that are all commonly referred to as world models. I will then focus specifically on video world models, a particular class of world models aimed at simulating the world through video prediction methods. Additionally, I will discuss why popular video generators (e.g., OpenAIâ€™s Sora or Google's Veo3) do not yet qualify as true world models, and outline a path toward bridging this gap within video generation frameworks.
            </p>

            <h2 id="what-are-world-models">What Are World Models? <br> Understanding versus Simulating the World</h2>

            <figure>
                <img src="../imgs/blog/world_model.jpg" alt="Description">
                <figcaption>Figure 1: An internal "world model" in the human brain that predicts the coarse future, versus an external "world model" that aims to simulate every detail of the reality. Figure modified from <a href="https://leshouches2022.github.io/SLIDES/compressed-yann-1.pdf">Yann LeCun's slides</a>.</figcaption>
            </figure>
            <p>
                In general, a world model predicts how the world will evolve given the current state and an action; in other words, it learns a function that takes a current world state and an action, and outputs the next state.Â However, there is no consensus on what constitutes this state. Some researchers define it as an abstract representation of the world, in which case the world model mirrors the cognitive world model in the human brain that predict events at a high, semantic level.
                Others, on the other hand, consider the world state to be a detailed and complete description of the world.
                In that case, the world model is a fully realistic world simulator, akin to <i>The Matrix</i>â€”a high-fidelity simulation virtually indistinguishable from reality. <strong>Despite their differing goals and methodologies, both paradigms are commonly referred to as "world models.</strong>"
            </p>

            <figure>
                <img src="../imgs/blog/taxonomy.jpg" alt="Description">
                <figcaption>Figure 2: A taxonomy of different definitions and approaches to world models.</figcaption>
            </figure>

            <p>
                The first type of world model requires architectures that internalize world knowledge and perform reasoning and prediction in a semantic space. While LLMs are regarded as a promising path, many <a href="https://sergeylevine.substack.com/p/language-models-in-platos-cave">argue</a> that its sole reliance on textual knowledge leaves critical gaps in their grasp of reality.
                <!-- As Sergey Levine <a href="https://sergeylevine.substack.com/p/language-models-in-platos-cave">analogizes</a> via Platoâ€™s Allegory of the Cave, an LLM sees only the shadowsâ€”textual tracesâ€”of real-world events. -->
                A truly grounded world model must go beyond text and engage with the world through vision, action, and interaction.
                Recent multimodal models like <a href="https://arxiv.org/abs/2402.08268">LWM</a>, which are trained on long-context text and video, show promise in this direction.
                Similarly, LeCunâ€™s JEPA series (<i>e.g.</i>, <a href="https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/">V-JEPA 2</a>) advances this goal by predicting masked abstract video representations.
            </p>
            <p>
                While simulating pixel-level details may not be essential for intelligenceâ€”<a href="https://www.facebook.com/share/p/1BBDeNBe8g/">as LeCun has argued</a>â€”there remain many applications that require high-fidelity world simulators. Realistic simulations enable robots to learn complex tasks without the cost and limitations of real-world interaction, and they can also provide human users with immersive, interactive experiences. In both cases, high-quality visuals are crucial: robots need detailed visual inputs to bridge the sim-to-real gap, while humans need realistic visuals to experience true immersion.
                In robotic learning, internal world understanding models and external simulation models can work togetherâ€”robots can plan and act using internal world models that are learned through interaction with external world models that simulate the environment.
            </p>

            <div class="highlight">
                <strong>Key Insight:</strong> "World model" is an overloaded term that can refer to either an internal world understanding model or an external world simulation model. Both models can work together in the context of embodied AI.
            </div>

            <p>
                Several approaches exist for building high-fidelity world simulators, including <strong>video world models</strong>, physics-based simulation platforms, and neural 3D/4D modeling techniques. In the following section, I will focus on video world models and outline a pathway from the current state-of-the-art video generation systems toward true world models.
            </p>

            <h2 id="video-world-models">Video World Models</h2>
            The past few years have seen significant progress in video generation, with state-of-the-art systems now capable of generating photorealistic details with physically realistic dynamics (such as Veo3 from Google). Demis Hassabis, CEO of Google DeepMind, hinted at the potential for developing an interactive world model with applications in gaming by leveraging video generation.

            <div style="display: flex; justify-content: center; padding-left: 200px; padding-right: 200px;">
                <blockquote class="twitter-tweet" data-theme="light">
                    <p lang="en" dir="ltr"></p>
                    <a href="https://twitter.com/demishassabis/status/1940248521111961988"></a>
                </blockquote>
            </div>
            <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

            <p>
                But why aren't current video generation models considered true world models yet? What precisely is lacking, and how can we bridge this gap? I argue that a true video world model must be <strong>causal, interactive, persistent, real-time, and physical accurate</strong>. In the following sections, I examine each property in detail, highlighting its importance and reviewing relevant research.
            </p>

            <h3 id="causality">Causal</h3>

            <div class="matrix-quote">
                <div class="quote-text">
                    "You see, there is only one constant, one universal, it is the only real truth: causality. Action. Reaction. Cause and effect."
                </div>
                <div class="quote-author">
                    â€” Merovingian, "The Matrix Reloaded"
                </div>
            </div>

            <p>
                Causality
                 <span id="causality-trigger" style="cursor: pointer; color: #007bff; font-weight: 500; margin-right: 4px;">
                     <span style="font-size: 0.9em; border: 1px solid #007bff; border-radius: 3px; padding: 2px 6px;">+</span>
                 </span>
                is a fundamental property of the universe.
                However, state-of-the-art video generation models predominantly employ diffusion transformers (DiTs), which generate a fixed set of frames simultaneously using non-causal, bidirectional attention. Such bidirectional attention allows future frame information to influence the past, violating the inherent temporal asymmetry of reality. As a result, after generating the initial frame, the entire video sequence becomes predetermined, eliminating the possibility for users to dynamically interact or influence subsequent events.
            </p>

             <!-- Tooltip for causality explanation -->
             <div id="causality-modal" style="display: none; position: absolute; z-index: 1000; background-color: white; padding: 20px; border-radius: 8px; max-width: 550px; box-shadow: 0 4px 12px rgba(0,0,0,0.15); border: 1px solid #ddd;">
                 <span id="causality-close" style="position: absolute; right: 8px; top: 5px; color: #aaa; font-size: 18px; font-weight: bold; cursor: pointer;">&times;</span>
                 <h4 style="margin: 0 0 10px 0; color: #2c3e50; font-size: 1.1em;">Causality</h4>
                 <p style="color: #666; line-height: 1.5; margin: 0; font-size: 0.9em;">
                     In this context, causality refers to the unidirectional nature of time, where the past determines the future, not vice versa. It differs from the concept in causal machine learning, where the focus is on distinguishing causation from correlation.
                 </p>
             </div>

            <p>
                Bringing causal dependencies and autoregressive, frame-by-frame generation capabilities to video generation systems is a <strong>first step towards true world models</strong>. Without it, real-time interaction is impossible.
                While LLM-style autoregressive models are inherently causal and have been applied to video generation, <i>e.g.</i>, <a href="https://arxiv.org/abs/2205.15868">CogVideo</a> and <a href="https://sites.research.google/videopoet/">VideoPoet</a>, their quality is severely limited due to the reliance on lossy vector quantization to obtain a discrete latent space. Also, generating one token at a time is slow and cannot be easily parallelized, making it very challenging to achieve real-time speed.
            </p>

            <p>
                <strong>It would be great if we can combine the quality/speed of diffusion models with the causality of autoregressive models.</strong>
                <a href="https://arxiv.org/abs/2407.01392">Diffusion Forcing</a> demonstrates that training diffusion models with per-frame independent noise levels enables autoregressive generation capabilities. Although Chen et al. primarily experiment with simple RNN architectures trained on synthetic datasets, <a href="https://causvid.github.io/">CausVid</a> shows that video diffusion transformers with bidirectional attention pretrained on internet-scale video datasets can be successfully adapted into autoregressive diffusion transformers (AR-DiT) with causal attention. <a href="https://github.com/SandAI-org/MAGI-1">MAGI-1</a> explores pretraining such AR-DiTs from scratch and proposes several infrastructure improvements designed for this type of architecture.
            </p>

            <figure>
                <img src="../imgs/blog/causvid.jpg" alt="Description">
                <figcaption>Figure 3: <a href="https://causvid.github.io/">CausVid</a> transforms a pretrained video diffusion model with bidirectional attention into a few-step, autoregressive video diffusion model with causal attention.</figcaption>
            </figure>

            <p>
                A causal generative model without interactive controllability is a passive world simulator operating in observer mode. To be genuinely useful in practice, a world model must support interactive controls, enabling real-time user or agent engagement with the simulated environment.
            </p>

            <h3 id="interactive">Interactive</h3>
            <p>
                 Interactive controllability is a defining characteristic for video world models. It refers to <strong>the ability to influence the future through actions that are injected on the fly.</strong>. The nature of actions differs across applications: a gamer might move characters and manipulate objects, a robot must execute motor commands supported by the hardware, while an autonomous vehicle makes steering and acceleration decisions. What unites these diverse scenarios is the fundamental requirement that the world model must respond dynamically to external interventions, adapting its predictions as new actions are introduced.
             </p>

             <p>
                <!-- Interactive controllability has been a defining feature for video world models.  -->
                Early approaches primarily explored game action controls and commonly employed recurrent variational autoencoders as generative world models  (<a href="https://worldmodels.github.io/">Ha and Schmidhuber 2018</a>; <a href="https://arxiv.org/abs/1912.01603">Hafner et al., 2020</a>; <a href="https://research.google/blog/mastering-atari-with-discrete-world-models/">Hafner et al., 2021</a>). More recently, research has shifted towards (autoregressive) diffusion models conditioned on interactive actions (<a href="https://gamengen.github.io/">Valevski et al., 2024</a>; <a href="https://diamond-wm.github.io/">Alonso et al., 2024</a>), often trained on increasingly realistic game environments (<a href="https://thematrix1999.github.io/">Feng et al., 2024</a>; <a href="https://gamegen-x.github.io/">Che et al., 2025</a>; <a href="https://hunyuan-gamecraft.github.io/">Li et al., 2025</a>). Other forms of application-specific action spaces have also been investigated, such as driving maneuvers in autonomous vehicles (<a href="https://wayve.ai/science/gaia/">Hu et al., 2023</a>) and locomotion actions in navigation robots (<a href="https://www.amirbar.net/nwm/">Bar et al., 2025</a>).
             </p>

             <p>
                A central challenge in enabling interactive controllability is acquiring training data that aligns video frames with corresponding actions. While annotated data can be sourced from gaming environments, leveraging unlabeled, internet-scale video for training interactive world models remains an open problem. <a href="https://sites.google.com/view/genie-2024/home">Genie</a> explored unsupervised action learning from unlabeled videos, and its successor, <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie-2</a> showed impressive results by leveraging AR-DiT architecture.
             </p>

             <figure>
                 <video autoplay loop muted style="width: 100%; max-width: 800px;">
                     <source src="../imgs/blog/genie.mp4" type="video/mp4">
                     Your browser does not support the video tag.
                 </video>
                 <figcaption>Figure 4: <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genie-2</a> generates different future conditioned on different interactive actions.</figcaption>
             </figure>

            <h3 id="persistence">Persistent</h3>
            <p>
                Generating just a few seconds of video is insufficient for most applications of world simulation. Ideally, a video world model should be capable of producing videos of indefinite lengthâ€”or at the very least, videos <strong>long enough</strong> to support meaningful tasks. More critically, the model must ensure <strong>consistency across time</strong>, maintaining coherence with its own prior generations. This property, which I refer to as persistence, is another essential requirement for video world models.
            </p>

            <p>
                At first glance, generating long, consistent videos might appear to be merely a scaling problem: in theory, a transformer with an extremely long context window, trained on sufficiently long-duration data, should be capable of maintaining temporal coherence. However, another critical requirement of a world modelâ€”real-time responsiveness (which I will discuss shortly)â€”complicates the issue. Unlike LLM applications where users can tolerate increased inference time as the context window grows, video world models face a hard latency constraint. For interactive applications such as games, it is unacceptable for the system to slow down over time. A world model that becomes increasingly laggy over time would make the experience unplayable.
            </p>

            <p>
                Researchers have proposed various techniques to address this challenge.
                One common strategy introduces an inductive bias that aggressively compresses temporally distant frames, as in <a href="https://farlongctx.github.io/">FAR</a> and <a href="https://lllyasviel.github.io/frame_pack_gitpage/">FramePack</a>. While it works well for scenarios where the recent frames contain sufficient information for prediction, it might fail when crucial details are buried in earlier framesâ€”resembling the "needle in a haystack" problem in LLMs.
                Another line of work integrates video generation with persistent 3D conditions (<a href="https://research.nvidia.com/labs/toronto-ai/GEN3C/">Ren et al., 2025</a>, <a href="https://zqh0253.github.io/wvd/">Zhang et al., 2025</a>, <a href="https://spmem.github.io/">Wu et al., 2025</a>, <a href="https://arxiv.org/abs/2505.05495">Zhou et al., 2024</a>).
                While effective in static scenes, these approaches struggle to model dynamic environments due to the complexity of representing explicit 4D structures.
                Finally, a promising direction is to use a linear RNNs, such as state-space models to capture long-range temporal dependencies without increasing per-frame generation time, as first explored by <a href="https://ryanpo.com/ssm_wm/">Po et al., 2025</a>. Their approach integrates a linear RNN with 3D hidden states and incorporates local attention mechanisms, which is reminiscent of classic <a href="https://proceedings.neurips.cc/paper_files/paper/2015/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf">ConvLSTM</a> architectures widely used for video prediction in the pre-transformer era.
            </p>

            <figure>
                <img src="../imgs/blog/sswm.png" alt="Description">
                <figcaption>Figure 5: <a href="https://ryanpo.com/ssm_wm/">State-Space Video World Model</a> integrates block-wise SSM scans with 3D hidden states to enable long-term memory, together with local attention mechanisms for enhanced visual fidelity.</figcaption>
            </figure>

            <h3 id="real-time">Real-Time</h3>
            <p>
                <!-- Real-time, interactive experience is an essential requirement for video world models, but what exactly does it mean to be real-time? There are two metrics that measure the efficiency of a video world model: throughput and latency. Throughput is the number of frames generated per second. A necessary condition for real-time experience is real-time throughput, which means that the model can generate a frame faster than people watch it. -->
                A video world model that aims to provide interactive experience for human users needs to run in real-time, but what exactly does "real-time" mean in this context? To assess whether a model operates in real-time, two metrics need to be considered: <strong>throughput</strong> and <strong>latency</strong>. Throughput refers to the number of frames generated per second by the model, while latency is the delay between each interactive action and the frame that responds to it. Achieving real-time throughputâ€”where the model generates frames at or above the rate that users consume themâ€”is a necessary condition for delivering a real-time experience.
            </p>

            <p>
                However, <strong>real-time throughput is not sufficient</strong>. The latency of the model must also be low enough to be imperceptible to the user. The latency generally needs to be less than a second, and the specific latency threshold varies across applications. For example, <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4654242">studies</a> have showed that the acceptable latency is about one second for live streaming, 0.1s for gaming, and 0.01s for virtual reality. And here is another reason why causal/autoregressive generation is so fundamental to video world models: a model with non-causal dependencies that generates a t-second video at a time has a minimum latency of t seconds in response to frame-wise interactive actions. Therefore, <strong>a non-causal diffusion model will never achieve truly real-time latency</strong>, regardless of how fast throughput the model can achieve.
            </p>

            <p>
                To enable real-time experience, it's crucial to integrate <strong>causal, frame-by-frame autoregressive modeling with a fast, few-step distilled diffusion</strong> process to generate each frame.
                <a href="https://causvid.github.io/">CausVid</a> was a pioneer in this direction, introducing AR-DiT trained with Distribution Matching Distillation (<a href="https://tianweiy.github.io/dmd/">DMD</a>).
                The follow-up work, <a href="https://self-forcing.github.io/">Self-Forcing</a>, further improves the quality and speed by simulating the inference process (autoregressive rollout with KV caching) during training. It achieves real-time throughput (17FPS) and subsecond latency on a single GPU, while matching the quality of state-of-the-art open-source video diffusion models. <a href="https://seaweed-apt.com/2">APT2</a> explores a similar idea of training-time rollout, and further demonstrates that such real-time video models can be finetuned with interactive controls.
                There have also been demos from startups such as <a href="https://oasis.decart.ai/welcome">Decart</a>, <a href="https://experience.odyssey.world/">Odyssey</a>, <a href="https://wayfarerlabs.ai/blog">Wayfarer Labs</a> and <a href="https://blog.dynamicslab.ai/">Dynamics Lab</a>, though overall the quality and generalizability still lag behind state-of-the-art video generation models.
            </p>

            <figure>
                <video autoplay loop muted style="width: 100%; max-width: 800px;">
                    <source src="../imgs/blog/self-forcing.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <figcaption>Figure 6: <a href="https://self-forcing.github.io/">Self-Forcing</a> generates photorealistic videos with real-time throughput and subsecond latency.</figcaption>
            </figure>


            <p></p>

            <h3 id="physically-accurate">Physically Accurate</h3>
            <p>
                The ultimate goal of video world models is to serve as a world simulator that is indistinguishable from reality. This requires the model  not only to generate visually pleasing pixels, but also to adhere to real-world physics. While the physical realism of these models generally improves as they are scaled with more data and parameters, it remains unclear whether current models can truly learn generalizable and extrapolative physical laws. <a href="https://phyworld.github.io/">Kang et al.</a> introduce a simple 2D dataset generated via a physics simulator and evaluate the generalization capabilities of video models trained on this data. They find that although combinatorial generalization is achievable, existing models still fail to generalize in extrapolative scenarios where the observed physical properties (<i>e.g.</i>, velocity, mass) are out of distribution.
            </p>

            <p>
                While this result seems to suggest a doomed future, it doesnâ€™t mean video world models canâ€™t effectively simulate the world in most practical scenarios. Unless you're trying to model cutting-edge physics experiments, the use cases are likely going to be compositionally similar to those found in large internet-scale video datasets. After all, if the training distribution covers the entire world, there is nothing out of distribution. ğŸ˜‰
                <!-- While this may sound like a doomed future, it doesn't mean we cannot use video models to simluate the world in most applications where there are compositionally similar scenarios in internet video datasets (unless you want to use video models to simulate frontier physics experiments). As long as the training distribution covers the entire world, there is nothing out of distribution ;). -->
            </p>

            <figure>
                <img src="../imgs/blog/ood.jpeg" alt="Description" style="padding-left: 200px; padding-right: 200px;">
                <figcaption>Figure 7: Meme from <a href="https://youtu.be/ZZC_xqRgcHo?t=1290">Tim Rocktaeschel's talk</a> on world models.</figcaption>
            </figure>

            <p>
                Beyond simply scaling up models, several techniques have been proposed to enhance the physical realism of video world models. <a href="https://hila-chefer.github.io/videojam-paper.github.io/">VideoJAM</a> argues that the commonly used pixel reconstruction objective in diffusion models is insufficient for capturing physical coherence. To address this, it introduces a joint training approach that incorporates an additional optical flow denoising objective.
                Another approach is dataset curation to emphasize rare, out-of-distribution events where physical accuracy is more important. For instance, <a href="https://anthonygosselin.github.io/Ctrl-Crash-ProjectPage/">Ctrl-Crash</a> fine-tunes a video diffusion model on car accident footage, while <a href="https://github.com/vision-x-nyu/pisa-experiments">PISA</a> finetunes it on synthetic or real-world object dropping videos.
            </p>

            <h3 id="summary">Summary</h3>

            <p>
                Iâ€™ve outlined five key properties of video world models: causality, interactivity, persistence, real-time responsiveness, and physical accuracy, roughly in the order from most fundamental to most challenging. The first twoâ€”causality and interactivityâ€”are hard constraints. A world model has to be causal and interactive, while causality is more fundamental, being the prerequisite for interactive controllability. Persistence is a soft constraint, existing on a spectrum; while the ideal world model needs to produce infinitely-long and fully consistent videos, even moderately persistent models are valuable in many applications. There are <strong>trade-offs between real-time responsiveness and physical accuracy</strong>, the two remaining properties: larger models tend to be more physically accurate but slower, while smaller models are necessary for real-time responsiveness but less physically accurate.
                I expect <strong>different applications of world modeling will prioritize one over the other</strong>. For human entertainment, real-time responsiveness is essential and physical accuracy only needs to be good enough to fool human eyes. For robotic learning, physical accuracy is the most important, while real-time simulation during model training is not truly necessary.
            </p>

            <figure>
                <img src="../imgs/blog/pyramid.jpg" alt="Description">
                <figcaption>Figure 8: Levels of video world models as a dual-head pyramid. There exists a trade-off between real-time responsiveness and physical accuracy, and I expect different applications will prioritize one over the other.</figcaption>
            </figure>

            <h2 id="other-approaches">Other Approaches to World Simulation</h2>
            <p>
                Video world models provide a promising new way to simulate the world, yet they are only one path. Conventional simulators couple a physics engine for dynamics with a renderer for imagery; platforms such as <a href="https://github.com/google-deepmind/mujoco">MuJoCo</a>, <a href="https://developer.nvidia.com/isaac">Isaac</a>, and <a href="https://github.com/Genesis-Embodied-AI/Genesis">Genesis</a> plug neatly into robotics-learning pipelines. Still, creating realistic scenes and task-specific worlds in these engines often involves substantial manual effort and limits flexibility. Recent works start to explore automated environment generation with LLM agents, potentially enabling rapid, on-demand creation of new scenarios.
            </p>

            <p>
                Another approach of world modeling is to use a neural network to generate explicit 3D/4D world representations such as 3D/4D Gaussian Splatting. Latest research in this space includes <a href="https://snap-research.github.io/wonderland/">Wonderland</a>, <a href="https://kovenyu.com/wonderworld/">WonderWorld</a>, <a href="https://szymanowiczs.github.io/bolt3d">Bolt3D</a>, and many others. Startup companies such as <a href="https://www.worldlabs.ai/blog">World Labs</a> and <a href="https://www.spaitial.ai/">SpAItial</a> also appear to have works on this direction.
            </p>

            <figure>
                <img src="../imgs/blog/table.jpg" alt="Description">
                <figcaption>Figure 9: Pros and cons of different world simulation approaches.</figcaption>
            </figure>

            <p>
                Different approaches offer distinct strengths and limitations. Physics-based simulators are more effective at producing consistent and physically realistic results, but they often require manual effort to design task-specific scenarios. Moreover, traditional rendering pipelines can struggle to deliver full photorealism within real-time computational constraints. Neural 3D methods generate 3D-consistent representations that are fast to render, yet they typically face challenges in modeling dynamic scenes. Meanwhile, video-based world models are highly adaptable to custom needs and capable of producing visually stunning, photorealistic outputs. However, achieving 3D consistency alongside real-time renderingâ€”what I previously referred to as the challenge of "persistence" and "real-time"â€”remains an open problem. In short term, it might be fruitful to develop hybrid models that combine the strengths of different paradigms. For example, <a href="https://kyleleey.github.io/WonderPlay/">WonderPlay</a> combines physics-based simulators, 3D Gaussian representations, and video diffusion models together to produce photorealistic worlds with physically accurate dynamics.
            </p>

            <p>
                <strong>Disclaimer:</strong> The author is not an expert in physics-based simulation or neural rendering. Please take my opinions here with a grain of salt.
            </p>

            <h2 id="conclusion">Conclusion</h2>
            <p>
                The term world model is overloaded: it can describe abstract models predicting high-level outcomes or detailed simulations indistinguishable from reality. In this blog post, I focus on the second definition, outlining the key properties required to transition from video generation to genuine video world modeling. This is an exciting time, as interactive media powered by world models promises to become the next frontier in content creation (following image and video generation), alongside transformative new paradigms in robotic learning enabled by world models.
            </p>

            <h2 id="citation">Citation</h2>
            <p>
                If you find this blog post useful, please consider citing:
            </p>
            <div style="text-align: left; background-color: #f8f9fa; border: 1px solid #e9ecef; border-radius: 8px; padding: 20px; margin: 20px 0; font-family: 'Courier New', monospace; font-size: 14px; line-height: 1.5; overflow-x: auto;">
                @misc{huang2025towards,<br>
                &nbsp;&nbsp;&nbsp;&nbsp;title={Towards Video World Models},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;author={Huang, Xun},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;year={2025},<br>
                &nbsp;&nbsp;&nbsp;&nbsp;url={https://www.xunhuang.me/blogs/world_model.html}<br>
                }
            </div>
            <h2 id="ackowledgements">Acknowledgements</h2>
            <p>
                I would like to thank Hong-Xing Yu and Beidi Chen for their valuable feedback.
            </p>

            <h2 id="references">References</h2>
            <ol>
                <li>Alonsoâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2405.12399">Diffusion for World Modeling:â€¯Visualâ€¯Detailsâ€¯Matterâ€¯inâ€¯Atari</a></li>
                <li>Assranâ€¯etâ€¯al.,â€¯2025, <a href="https://ai.meta.com/blog/v-jepa-2-world-model-benchmarks/">Vâ€‘JEPAâ€¯2:â€¯Videoâ€¯Worldâ€¯Modelâ€¯Benchmarks</a></li>
                <li>Barâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2412.03572">Navigationâ€¯Worldâ€¯Models</a></li>
                <li>Bruceâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2402.15391">Genie:â€¯Generativeâ€¯Interactiveâ€¯Environments</a></li>
                <li>Cheâ€¯etâ€¯al.,â€¯2025, <a href="https://gamegen-x.github.io/">GameGenâ€‘X:â€¯Interactiveâ€¯Openâ€‘worldâ€¯Gameâ€¯Videoâ€¯Generation</a></li>
                <li>Cheferâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2502.02492">VideoJAM:â€¯Jointâ€¯Appearanceâ€‘Motionâ€¯Representationsâ€¯forâ€¯Enhancedâ€¯Videoâ€¯Generation</a></li>
                <li>Chenâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2407.01392">Diffusion Forcing: Nextâ€‘tokenâ€¯Predictionâ€¯Meetsâ€¯Fullâ€‘Sequenceâ€¯Diffusion</a></li>
                <li>Fengâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2412.03568">Theâ€¯Matrix:â€¯Infiniteâ€‘Horizonâ€¯Worldâ€¯Generationâ€¯withâ€¯Realâ€‘Timeâ€¯Movingâ€¯Control</a></li>
                <li>Gosselinâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2506.00227">Ctrlâ€‘Crash:â€¯Controllableâ€¯Diffusionâ€¯forâ€¯Realisticâ€¯Carâ€¯Crashes</a></li>
                <li>Guâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2503.19325">Longâ€‘Contextâ€¯Autoregressiveâ€¯Videoâ€¯Modelingâ€¯withâ€¯Nextâ€‘Frameâ€¯Prediction</a></li>
                <li>Haâ€¯etâ€¯al.,â€¯2018, <a href="https://papers.nips.cc/paper/7512-recurrent-world-models-facilitate-policy-evolution">Recurrentâ€¯Worldâ€¯Modelsâ€¯Facilitateâ€¯Policyâ€¯Evolution</a></li>
                <li>Hafnerâ€¯etâ€¯al.,â€¯2020, <a href="https://arxiv.org/abs/1912.01603">Dreamâ€¯toâ€¯Control:â€¯Learningâ€¯Behaviorsâ€¯byâ€¯Latentâ€¯Imagination</a></li>
                <li>Hafnerâ€¯etâ€¯al.,â€¯2021, <a href="https://arxiv.org/abs/2010.02193">Masteringâ€¯Atariâ€¯withâ€¯Discreteâ€¯Worldâ€¯Models</a></li>
                <li>Hongâ€¯etâ€¯al.,â€¯2022, <a href="https://arxiv.org/abs/2205.15868">CogVideo:â€¯Largeâ€‘Scaleâ€¯Preâ€‘Trainingâ€¯forâ€¯Textâ€‘toâ€‘Videoâ€¯Generation</a></li>
                <li>Huâ€¯etâ€¯al.,â€¯2023, <a href="https://arxiv.org/abs/2309.17080">GAIAâ€‘1:â€¯Aâ€¯Generativeâ€¯Worldâ€¯Modelâ€¯forâ€¯Autonomousâ€¯Driving</a></li>
                <li>Huangâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2506.08009">Selfâ€‘Forcing:â€¯Bridgingâ€¯theâ€¯Trainâ€‘Testâ€¯Gapâ€¯inâ€¯Autoregressiveâ€¯Videoâ€¯Diffusion</a></li>
                <li>Kangâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2411.02385">Howâ€¯Farâ€¯Isâ€¯Videoâ€¯Generationâ€¯fromâ€¯Worldâ€¯Model?â€¯Aâ€¯Physicalâ€‘Lawâ€¯Perspective</a></li>
                <li>Kondratyukâ€¯etâ€¯al.,â€¯2023, <a href="https://sites.research.google/videopoet/">VideoPoet:â€¯Aâ€¯Simpleâ€¯Modelingâ€¯Methodâ€¯forâ€¯Highâ€‘Qualityâ€¯Videoâ€¯Generation</a></li>
                <li>Liâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2506.17201">Hunyuanâ€‘GameCraft:â€¯Highâ€‘Dynamicâ€¯Interactiveâ€¯Gameâ€¯Videoâ€¯Generationâ€¯withâ€¯Hybridâ€¯Historyâ€¯Condition</a></li>
                <li>Liâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2505.18151">WonderPlay:â€¯Dynamicâ€¯3Dâ€¯Sceneâ€¯Generationâ€¯fromâ€¯aâ€¯Singleâ€¯Imageâ€¯andâ€¯Actions</a></li>
                <li>Liâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/pdf/2503.09595">Pisa Experiments: Exploring Physics Post-Training for Video Diffusion Models by Watching Stuff Drop</a></li>
                <li>Liangâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2412.12091">Wonderland:â€¯Navigatingâ€¯3Dâ€¯Scenesâ€¯fromâ€¯aâ€¯Singleâ€¯Image</a></li>
                <li>Linâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2506.09350">Autoregressiveâ€¯Adversarialâ€¯Postâ€‘Trainingâ€¯forâ€¯Realâ€‘Timeâ€¯Interactiveâ€¯Videoâ€¯Generation</a></li>
                <li>Liuâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2402.08268">Worldâ€¯Modelâ€¯onâ€¯Millionâ€‘Lengthâ€¯Videoâ€¯andâ€¯Languageâ€¯withâ€¯Blockwiseâ€¯RingAttention</a></li>
                <li>Parkerâ€‘Holderâ€¯etâ€¯al.,â€¯2025, <a href="https://deepmind.google/discover/blog/genie-2-a-large-scale-foundation-world-model/">Genieâ€‘2:â€¯Aâ€¯Largeâ€‘Scaleâ€¯Foundationâ€¯Worldâ€¯Model</a></li>
                <li>Poâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2505.20171">Longâ€‘Contextâ€¯Stateâ€‘Spaceâ€¯Videoâ€¯Worldâ€¯Models</a></li>
                <li>Renâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2503.03751">GEN3C:â€¯3Dâ€‘Informedâ€¯Worldâ€‘Consistentâ€¯Videoâ€¯Generationâ€¯withâ€¯Preciseâ€¯Cameraâ€¯Control</a></li>
                <li>Szymanowiczâ€¯etâ€¯al.,â€¯2025, <a href="https://szymanowiczs.github.io/bolt3d">Bolt3D:â€¯Generatingâ€¯3Dâ€¯Scenesâ€¯inâ€¯Seconds</a></li>
                <li>Tengâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2505.13211">MAGIâ€‘1:â€¯Autoregressiveâ€¯Videoâ€¯Generationâ€¯atâ€¯Scale</a></li>
                <li>Valevskiâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2408.14837">Diffusionâ€¯Modelsâ€¯Areâ€¯Realâ€‘Timeâ€¯Gameâ€¯Engines</a></li>
                <li>Wuâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2506.05284">Videoâ€¯Worldâ€¯Modelsâ€¯withâ€¯Longâ€‘termâ€¯Spatialâ€¯Memory</a></li>
                <li>Yinâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2311.18828">Oneâ€‘Stepâ€¯Diffusionâ€¯withâ€¯Distributionâ€¯Matchingâ€¯Distillation</a></li>
                <li>Yinâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2412.07772">Fromâ€¯Slowâ€¯Bidirectionalâ€¯toâ€¯Fastâ€¯Autoregressiveâ€¯Videoâ€¯Diffusionâ€¯Models</a></li>
                <li>Yuâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2406.09394">WonderWorld:â€¯Interactiveâ€¯3Dâ€¯Sceneâ€¯Generationâ€¯fromâ€¯aâ€¯Singleâ€¯Image</a></li>
                <li>Zhangâ€¯etâ€¯al.,â€¯2024, <a href="https://arxiv.org/abs/2412.01821">Worldâ€‘Consistentâ€¯Videoâ€¯Diffusionâ€¯withâ€¯Explicitâ€¯3Dâ€¯Modeling</a></li>
                <li>Zhangâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2504.12626">Packingâ€¯Inputâ€¯Frameâ€¯Contextâ€¯inâ€¯Nextâ€‘Frameâ€¯Predictionâ€¯Modelsâ€¯forâ€¯Videoâ€¯Generation</a></li>
                <li>Zhouâ€¯etâ€¯al.,â€¯2025, <a href="https://arxiv.org/abs/2505.05495">Learningâ€¯3Dâ€¯Persistentâ€¯Embodiedâ€¯Worldâ€¯Models</a></li>

            </ol>
        <style>
            #references + ol,
            #references ~ ol {
                font-size: 0.95em;
            }
        </style>





        </div>

        <div class="toc-sidebar">
            <div class="toc-title">Table of Contents</div>
            <ul class="toc-list">
                <li><a href="#what-are-world-models" class="toc-h2">What Are World Models?</a>
                </li>
                <li><a href="#video-world-models" class="toc-h2">Requirements for Video World Models</a>
                    <ul>
                        <li><a href="#causality" class="toc-h3">Causal</a></li>
                        <li><a href="#interactive" class="toc-h3">Interactive</a></li>
                        <li><a href="#persistence" class="toc-h3">Persistent</a></li>
                        <li><a href="#real-time" class="toc-h3">Real-Time</a></li>
                        <li><a href="#physically-accurate" class="toc-h3">Physically Accurate</a></li>
                        <li><a href="#summary" class="toc-h3">Summary</a></li>
                    </ul>
                </li>
                <li><a href="#other-approaches" class="toc-h2">Other Approaches to World Simulation</a>
                </li>
                <li><a href="#conclusion" class="toc-h2">Conclusion</a></li>
            </ul>
        </div>
    </div>

    <script>
        // Add smooth scrolling for table of contents links
        document.querySelectorAll('.toc-list a').forEach(link => {
            link.addEventListener('click', function(e) {
                e.preventDefault();
                const targetId = this.getAttribute('href').substring(1);
                const targetElement = document.getElementById(targetId);
                if (targetElement) {
                    targetElement.scrollIntoView({
                        behavior: 'smooth',
                        block: 'start'
                    });
                }
            });
        });

        // Add IDs to headings for navigation
        document.querySelectorAll('h2, h3').forEach((heading, index) => {
            if (!heading.id) {
                heading.id = heading.textContent.toLowerCase().replace(/[^a-z0-9]+/g, '-').replace(/^-|-$/g, '');
            }
        });

        // Tooltip functionality
        const tooltip = document.getElementById('causality-modal');
        const trigger = document.getElementById('causality-trigger');
        const closeBtn = document.getElementById('causality-close');

        trigger.addEventListener('click', function(event) {
            event.stopPropagation();

            // Get the position of the trigger element
            const rect = trigger.getBoundingClientRect();
            const scrollTop = window.pageYOffset || document.documentElement.scrollTop;
            const scrollLeft = window.pageXOffset || document.documentElement.scrollLeft;

            // Position the tooltip near the trigger
            tooltip.style.left = (rect.left + scrollLeft + 30) + 'px';
            tooltip.style.top = (rect.top + scrollTop + 25) + 'px';
            tooltip.style.display = 'block';
        });

        closeBtn.addEventListener('click', function() {
            tooltip.style.display = 'none';
        });

        // Close tooltip when clicking outside of it
        document.addEventListener('click', function(event) {
            if (!tooltip.contains(event.target) && event.target !== trigger) {
                tooltip.style.display = 'none';
            }
        });
    </script>
</body>
</html>
