<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0">
    <title>Xun Huang's Academic Homepage</title>
    <link rel="stylesheet" href="styles.css">
</head>
<body>
    <section id="profile">
        <div class="container">
            <div class="profile-left">
                <img src="imgs/profile-photo.jpg" alt="Xun Huang Profile Photo" class="profile-photo">
            </div>
            <div class="profile-right">
                <h1>Xun Huang <span style="font-size: 1rem; font-weight: normal; color: #555;">(/shuun hwang/)</span></h1>
                <p>
                    <a href="mailto:xunhuang1995@gmail.com">Email</a> |
                    <a href="https://scholar.google.com/citations?user=1XGC4GsAAAAJ&hl=en&oi=ao">Google Scholar</a> |
                    <a href="https://x.com/xunhuang1995">Twitter/X</a> |
                    <a href="https://github.com/xunhuang1995">GitHub</a> |
                    <a href="#publications">Publications</a> |
                    <a href="#teaching">Teaching</a> |
                    <a href="#blog">Blog</a>
                </p>
            </div>
        </div>
    </section>

    <!-- Bio Section -->
    <section id="bio">
        <div class="container-large">
            <p>
                I was previously a Research Scientist at Adobe Research, an Adjunct Professor at CMU, and a Research Scientist at NVIDIA. I obtained my PhD in Computer Science from Cornell in 2020, advised by Professor <a href="https://www.belongielab.org/">Serge Belongie</a>.
            </p>
            <br>
            <p>
                I invented architectures and algorithms that have enabled autoregressive real-time video generation, including <a href="https://self-forcing.github.io/">Self Forcing</a> and Autoregressive Diffusion Transformers (<a href="https://causvid.github.io/">CausVid</a>). Previously, I developed one of the first public text-to-image demo (<a href="https://blogs.nvidia.com/blog/gaugan2-ai-art-demo/">GauGAN2</a>), as well as NVIDIA's first <a href="https://arxiv.org/abs/2211.01324">text-to-image</a> and <a href="https://arxiv.org/abs/2211.10440">text-to-3D</a> foundation models. My research has been cited over 17,000 times as of Dec 2025.

            </p>
            <br>
            <p>
                I have been working on multimodal "Generative AI" for 10 years. During my PhD, I invented Adaptive Instance Normalization (<a href="https://arxiv.org/abs/1703.06868">AdaIN</a>) and was <a href="https://arxiv.org/abs/1804.04732">the first to demonstrate</a> its effectiveness in generative neural networks. AdaIN became a foundational component of <a href="https://arxiv.org/abs/1812.04948">StyleGAN</a> and played a key role in <a href="https://arxiv.org/abs/1907.05600">the first working diffusion model</a>. Variants of AdaIN are now used in nearly all diffusion models.
                My PhD research was supported by Adobe Research Fellowship (2019), Snap Research Fellowship (2019), and NVIDIA Graduate Fellowship (2018).
            </p>
            <br>
        </div>
    </section>

    <!-- Publications Section -->
    <section id="publications">
        <div class="container">
            <h2>Selected/Recent Publications</h2>
            <div class="publication">
                <video src="imgs/ocean-book_compressed.mp4" alt="MotionStream" class="publication-image" autoplay loop muted playsinline></video>
                <div class="publication-details">
                    <h3>MotionStream: Real-Time Video Generation with Interactive Motion Controls</h3>
                    <p><em>ICLR 2026 (oral)</em></p>
                    <p>Joonghyuk Shin, Zhengqi Li, Richard Zhang, Jun-Yan Zhu, Jaesik Park, Eli Shechtman, <strong><u>Xun Huang</u></strong></p>
                    <a href="https://arxiv.org/abs/2511.01266">[arXiv]</a>
                    <a href="https://joonghyuk.com/motionstream-web/">[Project]</a>
                </div>
            </div>

            <div class="publication">
                <img src="https://nupurkmr9.github.io/npedit/static/images/method.jpg" alt="NP-Edit" class="publication-image">
                <div class="publication-details">
                    <h3>Learning an Image Editing Model without Image Editing Pairs</h3>
                    <p><em>ICLR 2026</em></p>
                    <p>Nupur Kumari, Sheng-Yu Wang, Nanxuan Zhao, Yotam Nitzan, Yuheng Li, Krishna Kumar Singh, Richard Zhang, Eli Shechtman, Jun-Yan Zhu, <strong><u>Xun Huang</u></strong></p>
                    <a href="https://arxiv.org/abs/2510.14978">[arXiv]</a>
                    <a href="https://nupurkmr9.github.io/npedit/">[Project]</a>
                </div>
            </div>

            <div class="publication">
                <video src="https://self-forcing.github.io/static/videos/demo.mp4" alt="Self-Forcing" class="publication-image" autoplay loop muted playsinline></video>
                <div class="publication-details">
                    <h3>Self Forcing: Bridging the Train-Test Gap in Autoregressive Video Diffusion</h3>
                    <p><em>NeurIPS 2025 (spotlight)</em></p>
                    <p><strong><u>Xun Huang</u></strong>, Zhengqi Li, Guande He, Mingyuan Zhou, Eli Shechtman</p>
                    <a href="https://arxiv.org/abs/2506.08009">[arXiv]</a>
                    <a href="https://self-forcing.github.io/ ">[Project]</a>
                    <a href="https://github.com/guandeh17/Self-Forcing ">[Code]</a>

                </div>
            </div>

            <div class="publication">
                <img src="imgs/sswm.gif" alt="SSWM" class="publication-image">
                <div class="publication-details">
                    <h3>Long-Context State-Space Video World Models</h3>
                    <p><em>ICCV 2025</em></p>
                    <p>Ryan Po, Yotam Nitzan, Richard Zhang, Berlin Chen, Tri Dao, Eli Shechtman, Gordon Wetzstein, <strong><u>Xun Huang</u></strong></p>
                    <a href="https://arxiv.org/abs/2505.20171">[arXiv]</a>
                    <a href="https://ryanpo.com/ssm_wm/ ">[Project]</a>
                </div>
            </div>

            <div class="publication">
                <img src="imgs/xfusion.jpg" alt="SSWM" class="publication-image">
                <div class="publication-details">
                    <h3>X-Fusion: Introducing New Modality to Frozen Large Language Models</h3>
                    <p><em>ICCV 2025 (Best Paper @ CVPR 2025 T4V Workshop)</em></p>
                    <p>Sicheng Mo, Thao Nguyen, <strong><u>Xun Huang</u></strong>, Siddharth Srinivasan Iyer, Yijun Li, Yuchen Liu, Abhishek Tandon, Eli Shechtman, Krishna Kumar Singh, Yong Jae Lee, Bolei Zhou, Yuheng Li</p>
                    <a href="https://arxiv.org/abs/2504.20996">[arXiv]</a>
                    <a href="https://sichengmo.github.io/XFusion/ ">[Project]</a>
                </div>
            </div>

            <div class="publication">
                <img src="imgs/causvid.png" alt="CausVid" class="publication-image">
                <div class="publication-details">
                    <h3>From Slow Bidirectional to Fast Autoregressive Video Diffusion Models</h3>
                    <p><em>CVPR 2025</em></p>
                    <p>Tianwei Yin*, Qiang Zhang*, Richard Zhang, William T. Freeman, Fredo Durand, Eli Shechtman, <strong><u>Xun Huang</u></strong></p>
                    <a href="https://arxiv.org/abs/2412.07772">[arXiv]</a>
                    <a href="https://causvid.github.io/ ">[Project]</a>
                    <a href="https://github.com/tianweiy/CausVid ">[Code]</a>
                </div>
            </div>


            <!-- <div class="publication">
                <img src="imgs/jedi.jpeg" alt="Jedi" class="publication-image">
                <div class="publication-details">
                    <h3>JeDi: Joint-Image Diffusion Models for Finetuning-Free Personalized Text-to-Image Generation</h3>
                    <p><em>CVPR, 2024</em></p>
                    <p>Yu Zeng, Vishal M Patel, Haochen Wang, Xun Huang, Ting-Chun Wang, Ming-Yu Liu, Yogesh Balaji</p>
                    <a href="https://arxiv.org/abs/2407.06187">[arXiv]</a>
                    <a href="https://research.nvidia.com/labs/dir/jedi/">[Project]</a>
                </div>
            </div> -->

            <!-- <div class="publication">
                <img src="imgs/diffcollage.jpg" alt="DiffCollage" class="publication-image">
                <div class="publication-details">
                    <h3>DiffCollage: Parallel Generation of Large Content with Diffusion Models</h3>
                    <p><em>CVPR, 2023</em></p>
                    <p>Qinsheng Zhang, Jiaming Song, Xun Huang, Yongxin Chen, Ming-Yu Liu</p>
                    <a href="https://arxiv.org/abs/2303.17076">[arXiv]</a>
                    <a href="https://research.nvidia.com/labs/dir/diffcollage/">[Project]</a>
                </div>
            </div> -->

        <div class="publication">
            <img src="imgs//magic3d.jpg" alt="Magic3D" class="publication-image">
            <div class="publication-details">
                <h3>Magic3D: High-Resolution Text-to-3D Content Creation</h3>
                <p><em>CVPR 2023 (Highlight)</em></p>
                <p>Chen-Hsuan Lin*, Jun Gao*, Luming Tang*, Towaki Takikawa*, Xiaohui Zeng*, <strong><u>Xun Huang</u></strong>, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, Tsung-Yi Lin</p>
                <a href="https://arxiv.org/abs/2211.10440">[arXiv]</a>
                <a href="https://research.nvidia.com/labs/dir/magic3d">[Project]</a>
                <a href="https://youtu.be/F1ZNshgvWOA?si=inUjwyPkszrHbm6x">[Video]</a>
            </div>
        </div>

        <div class="publication">
            <img src="imgs//ediffi.jpg" alt="eDiff-I" class="publication-image">
            <div class="publication-details">
                <h3>eDiff-I: Text-to-Image Diffusion Models with Ensemble of Expert Denoisers</h3>
                <p><em>arXiv 2022</em></p>
                <p>Yogesh Balaji, Seungjun Nah, <strong><u>Xun Huang</u></strong>, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, Tero Karras, Ming-Yu Liu</p>
                <a href="https://arxiv.org/abs/2211.01324">[arXiv]</a>
                <a href="https://research.nvidia.com/labs/dir/eDiff-I">[Project]</a>
                <a href="https://www.youtube.com/watch?v=WbaVvlgxbl4">[Video]</a>
            </div>
        </div>

        <div class="publication">
            <img src="imgs//poegan_full.jpg" alt="PoE-GANs" class="publication-image">
            <div class="publication-details">
                <h3>Multimodal Conditional Image Synthesis with Product-of-Experts GANs</h3>
                <p><em>ECCV 2022</em></p>
                <p><strong><u>Xun Huang</u></strong>, Arun Mallya, Ting-Chun Wang, Ming-Yu Liu</p>
                <a href="https://arxiv.org/abs/2112.05130">[arXiv]</a>
                <a href="https://research.nvidia.com/labs/dir/PoE-GAN/">[Project]</a>
                <a href="https://www.youtube.com/watch?app=desktop&v=56aA_FaeAPY">[Video]</a>
                <a href="https://youtu.be/Wbid5rvCGos?si=wcx8AjhvraHhXCCB">[Two Minute Papers]</a>
            </div>
        </div>

        <!-- <div class="publication">
            <img src="imgs//gan_survey.jpg" alt="GAN Survey" class="publication-image">
            <div class="publication-details">
                <h3>Generative Adversarial Networks for Image and Video Synthesis: Algorithms and Applications</h3>
                <p><em>Proceedings of the IEEE 2021</em></p>
                <p>Xun Huang, Arun Mallya, Ting-Chun Wang, Ming-Yu Liu</p>
                <a href="https://arxiv.org/abs/2008.02793">[arXiv]</a>
            </div>
        </div> -->

        <div class="publication">
            <img src="imgs//pointflow.gif" alt="PointFlow" class="publication-image">
            <div class="publication-details">
                <h3>PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows</h3>
                <p><em>ICCV 2019 (Oral)</em></p>
                <p>Guandao Yang*, <strong><u>Xun Huang</u></strong>*, Zekun Hao, Ming-Yu Liu, Serge Belongie, Bharath Hariharan</p>
                <a href="https://arxiv.org/abs/1906.12320">[arXiv]</a>
                <a href="https://github.com/stevenygd/PointFlow">[Code]</a>
                <a href="https://www.youtube.com/watch?v=jqBiv77xC0M">[Video]</a>
            </div>
        </div>

        <!-- <div class="publication">
            <img src="imgs//funit.gif" alt="FUNIT" class="publication-image">
            <div class="publication-details">
                <h3>Few-shot Unsupervised Image-to-Image Translation</h3>
                <p><em>ICCV 2019</em></p>
                <p>Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, Jan Kautz</p>
                <a href="https://arxiv.org/abs/1905.01723">[arXiv]</a>
                <a href="https://github.com/NVlabs/FUNIT/">[Code]</a>
                <a href="https://www.youtube.com/watch?v=kgPAqsC8PLM">[Video]</a>
            </div>
        </div> -->

        <div class="publication">
            <img src="imgs//munit.jpg" alt="MUNIT" class="publication-image">
            <div class="publication-details">
                <h3>Multimodal Unsupervised Image-to-Image Translation</h3>
                <p><em>ECCV 2018</em></p>
                <p><strong><u>Xun Huang</u></strong>, Ming-Yu Liu, Serge Belongie, Jan Kautz</p>
                <a href="https://arxiv.org/abs/1804.04732">[arXiv]</a>
                <a href="https://github.com/NVlabs/MUNIT">[Code]</a>
                <a href="https://youtu.be/ab64TWzWn40?si=w6sb-t2krah7Dtj-">[Video]</a>
            </div>
        </div>

        <!-- <div class="publication">
            <img src="imgs//captioning.jpg" alt="Image Captioning" class="publication-image">
            <div class="publication-details">
                <h3>Learning to Evaluate Image Captioning</h3>
                <p><em>CVPR 2018</em></p>
                <p>Yin Cui, Guandao Yang, Andreas Veit, Xun Huang, Serge Belongie</p>
                <a href="https://arxiv.org/abs/1806.06422">[arXiv]</a>
                <a href="https://github.com/richardaecn/cvpr18-caption-eval">[Code]</a>
            </div>
        </div> -->

        <!-- <div class="publication">
            <img src="imgs//videogen.jpg" alt="Controllable Video Generation" class="publication-image">
            <div class="publication-details">
                <h3>Controllable Video Generation with Sparse Trajectories</h3>
                <p><em>CVPR 2018</em></p>
                <p>Zekun Hao, Xun Huang, Serge Belongie</p>
                <a href="https://openaccess.thecvf.com/content_cvpr_2018/papers/Hao_Controllable_Video_Generation_CVPR_2018_paper.pdf">[PDF]</a>
                <a href="https://github.com/zekunhao1995/ControllableVideoGen">[Code]</a>
            </div>
        </div> -->

        <div class="publication">
            <img src="imgs//adain.jpg" alt="AdaIN" class="publication-image">
            <div class="publication-details">
                <h3>Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization</h3>
                <p><em>ICCV 2017 (Oral)</em></p>
                <p><strong><u>Xun Huang</u></strong>, Serge Belongie</p>
                <a href="https://arxiv.org/abs/1703.06868">[arXiv]</a>
                <a href="https://github.com/xunhuang1995/AdaIN-style">[Code]</a>
            </div>
        </div>

        <div class="publication">
            <img src="imgs//sgan.png" alt="SGAN" class="publication-image">
            <div class="publication-details">
                <h3>Stacked Generative Adversarial Networks</h3>
                <p><em>CVPR 2017</em></p>
                <p><p><strong><u>Xun Huang</u></strong>, Yixuan Li, Omid Poursaeed, John Hopcroft, Serge Belongie</p>
                <a href="https://arxiv.org/abs/1612.04357">[arXiv]</a>
                <a href="https://github.com/xunhuang1995/SGAN">[Code]</a>
            </div>
        </div>

        * indicates equal contribution.
        <br>
        See <a href="https://scholar.google.com/citations?user=1XGC4GsAAAAJ&hl=en&oi=ao">Google Scholar</a> for the full list of publications.

        <!-- <div class="publication">
            <img src="imgs//convpp.png" alt="Convolutional Pseudoprior" class="publication-image">
            <div class="publication-details">
                <h3>Top-Down Learning for Structured Labeling with Convolutional Pseudoprior</h3>
                <p><em>ECCV 2016</em></p>
                <p>Saining Xie*, Xun Huang*, Zhuowen Tu (*equal contribution)</p>
                <a href="http://arxiv.org/abs/1511.07409">[arXiv]</a>
            </div>
        </div> -->

        <!-- <div class="publication">
            <img src="imgs//salicon.jpg" alt="SALICON" class="publication-image">
            <div class="publication-details">
                <h3>SALICON: Reducing the Semantic Gap in Saliency Prediction by Adapting Deep Neural Networks</h3>
                <p><em>ICCV 2015</em></p>
                <p>Xun Huang, Chengyao Shen, Xavier Boix, Qi Zhao</p>
                <a href="https://openaccess.thecvf.com/content_iccv_2015/papers/Huang_SALICON_Reducing_the_ICCV_2015_paper.pdf">[PDF]</a>
            </div>
        </div> -->

    </section>

    <!-- Research Section -->
    <section id="teaching">
        <div class="container">
            <h2>Teaching</h2>
            <ul>
                <li>Instructor, <a href="https://cmu-dgm.github.io/">Deep Generative Models (18789)</a>, CMU, 2025 Spring</li>
                <li>Teaching Assistant, <a href="https://cornelltech.github.io/cs5785-fall-2019/index.html">Applied Machine Learning (CS 5785)</a>, Cornell Tech, 2017 Fall and 2019 Fall</li>
                <li>Teaching Assistant, <a href="https://www.cs.cornell.edu/courses/cs4320/2016fa/">Introduction to Database Systems (CS 4320)</a>, Cornell, Fall 2016</li>
            </ul>
        </div>
    </section>

    <!-- Blog Section -->
    <section id="blog">
        <div class="container">
            <h2>Blog</h2>
            <ul>
                <li><a href="blogs/world_model.html">Towards Video World Models</a></li>
            </ul>
        </div>
    </section>


    <!-- Collaborators Section -->
    <section id="collaborators">
        <div class="container">
            <h2>Student mentees/interns</h2>
            <p>I have been fortunate to work with many talented students and interns:</p>
            <ul>
                <li><a href="https://guandehe.github.io/">Guande He</a>, UT Austin</li>
                <li><a href="https://xingjianbai.com/">Xingjian Bai</a>, MIT</li>
                <li><a href="https://gsunshine.github.io/">Zhengyang Geng</a>, CMU</li>
                <li><a href="https://nupurkmr9.github.io/">Nupur Kumari</a>, CMU</li>
                <li><a href="https://gauravparmar.com/">Gaurav Parmar</a>, CMU</li>
                <li><a href="https://joonghyuk.com/">Joonghyuk Shin</a>, SNU</li>
                <li><a href="https://ryanpo.com/">Po Ryan</a>, Stanford</li>
                <li><a href="https://tianweiy.github.io/">Tianwei Yin</a>, MIT, now at Reve</li>
                <li><a href="https://sbyebss.github.io/">Jiaojiao Fan</a>, Georgia Tech, now at NVIDIA</li>
                <li><a href="https://zengxianyu.github.io/">Yu Zeng</a>, JHU, now at NVIDIA</li>
                <li><a href="https://qsh-zh.github.io/">Qinsheng Zhang</a>, Georgia Tech, now at NVIDIA</li>
                <li><a href="https://zinanlin.me/">Zinan Lin</a>, CMU, now at MSR</li>
                <li><a href="https://www.guandaoyang.com/">Guandao Yang</a>, Cornell, now at Apple</li>
                <li><a href="https://zekunhao.com/">Zekun Hao</a>, Beihang/Cornell, now at NVIDIA</li>
            </ul>
        </div>
    </section>
</body>
</html>
